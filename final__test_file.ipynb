{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final__test_file.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7umFXXDt0QJN"
      },
      "source": [
        "## 주의\n",
        "- 소수점으로 표현된 params들 숫자들 더 추가해주세요\n",
        "- randint(최소값, 최대값) 범위 조절도 해주세요 \n",
        "- 추가하시고싶은 선형회귀 모델있으면 알려주세요\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4iUd_kgj4XT"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler, RobustScaler, MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, VotingRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "from scipy.stats import randint"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYt3TWW5Iq1K"
      },
      "source": [
        "def model_scaler(data, col, scaler = None):\n",
        "  \n",
        "  '''\n",
        "  정규화 함수\n",
        "  data : dataframe\n",
        "  column : P_PRICE\n",
        "  scaler : standard, robust, minmax, log\n",
        "\n",
        "  '''\n",
        " \n",
        "  features = data.drop(col, axis=1)\n",
        "  target = data[col]\n",
        "\n",
        "  if scaler == 'standard':\n",
        "    scaler = StandardScaler()\n",
        "    features = scaler.fit_transform(features)\n",
        "\n",
        "    return features, target\n",
        "\n",
        "  elif scaler == 'robust':\n",
        "    scaler = RobustScaler()\n",
        "    features = scaler.fit_transform(features)\n",
        "\n",
        "    return features, target\n",
        "\n",
        "  elif scaler == 'minmax':\n",
        "    scaler = MinMaxScaler()\n",
        "    features = scaler.fit_transform(features)\n",
        "\n",
        "    return features, target\n",
        "\n",
        "  elif scaler == 'log':\n",
        "    features = np.log1p(features)\n",
        "\n",
        "    return features, target\n",
        "\n",
        "  elif scaler == 'None':\n",
        "\n",
        "    return features, target\n",
        "\n",
        "\n",
        "################################################################################################################################################\n",
        "\n",
        "def model_train(data, col, scaler, model = None):\n",
        "\n",
        "  '''\n",
        "  \n",
        "  data : dataframe\n",
        "  column : P_PRICE\n",
        "  scaler : standard, robust, minmax, log\n",
        "  model_name : linear, ridge, lasso, elastic, decisiontree,\n",
        "               randomforest, ada, gradient, xgb, lgbm\n",
        "\n",
        "  '''\n",
        "\n",
        "  features, target = model_scaler(data, col, scaler)\n",
        "  x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=0)\n",
        "  \n",
        "  if model == 'linear': \n",
        "    \n",
        "    model = LinearRegression()\n",
        "    neg_mse_scores = cross_val_score(model, features, target, scoring = 'neg_mean_squared_error', cv = 5)\n",
        "    rmse_scores = np.sqrt(-1 * neg_mse_scores)\n",
        "    avg_rmse = np.mean(rmse_scores)\n",
        "\n",
        "\n",
        "    print('RMSE : {:.4f}'.format(avg_rmse))\n",
        "\n",
        "  elif model == 'ridge':\n",
        "    \n",
        "    params = {\n",
        "              'alpha': (0.01, 0.0001, 0.003, 0.5, 0.04, 0.008, 0.001, 0.351, 0.096, 0.853, 0.185, 0.01825, 0.012385, 0.1, 1, 10, 100, 200, 50, 30, 20, 29, 58),            \n",
        "              'fit_intercept':(True, False),\n",
        "              'normalize':(True, False),\n",
        "\n",
        "              }\n",
        "\n",
        "    ridge = Ridge(random_state=0)\n",
        "    final = RandomizedSearchCV(ridge, param_distributions = params, cv = 5, scoring = 'neg_mean_squared_error', n_iter = 1000, n_jobs = -1 ,random_state=0)\n",
        "    final.fit(x_train, y_train)\n",
        "    pred = final.predict(x_test)\n",
        "\n",
        "    print('Best Params:', final.best_params_)\n",
        "    print('Best Score:', np.sqrt(-1 *final.best_score_))\n",
        "    print('Predict RMSE:',(np.sqrt(mean_squared_error(y_test, pred))))\n",
        "    \n",
        "  elif model == 'lasso':\n",
        "\n",
        "    params = {\n",
        "              'alpha': (0.01, 0.0001, 0.003, 0.5, 0.04, 0.008, 0.001, 0.351, 0.096, 0.853, 0.185, 0.01825, 0.012385, 0.1, 1, 10, 100, 200, 50, 30, 20, 29, 58),            \n",
        "              'fit_intercept':(True, False),\n",
        "              'normalize':(True, False),\n",
        "\n",
        "              }\n",
        "\n",
        "    lasso = Lasso(random_state=0)\n",
        "    final = RandomizedSearchCV(lasso, param_distributions = params, cv = 5, scoring = 'neg_mean_squared_error', n_iter = 1000, n_jobs = -1 ,random_state=0)\n",
        "    final.fit(features, target)\n",
        "    pred = final.predict(x_test)\n",
        "\n",
        "    print('Best Params:', final.best_params_)\n",
        "    print('Best Score:', np.sqrt(-1 *final.best_score_))\n",
        "    print('Predict RMSE:',(np.sqrt(mean_squared_error(y_test, pred))))\n",
        "  \n",
        "  elif model == 'elastic':\n",
        "    \n",
        "\n",
        "    params = {\n",
        "       'alpha': (0.1, 0.01, 0.5, 1, 3, 5, 10),\n",
        "       'l1_ratio':(0.01, 0.0001, 0.003, 0.5, 0.04, 0.008, 0.1, 0.1)\n",
        "    }\n",
        "\n",
        "    elastic = ElasticNet()\n",
        "    final = RandomizedSearchCV(elastic, param_distributions = params, cv = 5, scoring = 'neg_mean_squared_error', n_iter = 1000, n_jobs = -1 ,random_state=0)\n",
        "    final.fit(features, target)\n",
        "    pred = final.predict(x_test)\n",
        "\n",
        "    print('Best Params:', final.best_params_)\n",
        "    print('Best Score:', np.sqrt(-1 *final.best_score_))\n",
        "    print('Predict RMSE:',(np.sqrt(mean_squared_error(y_test, pred))))\n",
        "  elif model == 'decisiontree':\n",
        "    \n",
        "    params = {\n",
        "              'max_depth': randint(10, 200),            \n",
        "               #'min_child_samples': randint(5, 50),\n",
        "              'min_samples_split':randint(1, 200),\n",
        "              'min_samples_leaf': randint(1, 200),\n",
        "              \n",
        "    }\n",
        "\n",
        "    dt = DecisionTreeRegressor(random_state=0)\n",
        "    final = RandomizedSearchCV(dt, param_distributions = params, cv = 5, scoring = 'neg_mean_squared_error', n_iter = 1000, n_jobs = -1 ,random_state=0)\n",
        "    final.fit(features, target)\n",
        "    pred = final.predict(x_test)\n",
        "\n",
        "    print('Best Params:', final.best_params_)\n",
        "    print('Best Score:', np.sqrt(-1 *final.best_score_))\n",
        "    print('Predict RMSE:',(np.sqrt(mean_squared_error(y_test, pred))))\n",
        "  elif model == 'randomforest':\n",
        "    \n",
        "    params = {\n",
        "              'max_depth': randint(30, 1000),            \n",
        "              'n_estimators':randint(100, 1000),\n",
        "               #'min_child_samples': randint(5, 50),\n",
        "              'min_samples_leaf':randint(1, 1000),\n",
        "              'min_samples_split': randint(2, 1000),\n",
        "              'max_leaf_nodes': randint(2, 1000)\n",
        "\n",
        "              }\n",
        "\n",
        "    rf = RandomForestRegressor(random_state=0)\n",
        "    final = RandomizedSearchCV(rf, param_distributions = params, cv = 5, scoring = 'neg_mean_squared_error', n_iter = 1000, n_jobs = -1 ,random_state=0)\n",
        "    final.fit(features, target)\n",
        "    pred = final.predict(x_test)\n",
        "\n",
        "    print('Best Params:', final.best_params_)\n",
        "    print('Best Score:', np.sqrt(-1 *final.best_score_))\n",
        "    print('Predict RMSE:',(np.sqrt(mean_squared_error(y_test, pred))))\n",
        "\n",
        "\n",
        "  elif model == 'gradinet':\n",
        "\n",
        "    params = {'n_estimators' : randint(30, 1000),\n",
        "              'learning_rate' :(0.01, 0.0001, 0.003, 0.5, 0.04, 0.008, 0.001, 0.351, 0.096, 0.853, 0.185, 0.01825, 0.012385, 0.1),\n",
        "              'subsample' : (0.01, 0.1, 0.5, 0.08, 0.35, 0.3, 0.001, 0.03, 0.006, 0.153, 0.193, 0.0012, 0.0083 ,1),\n",
        "              'min_samples_split' : randint(2, 1000),\n",
        "              'max_depth' : randint(3, 1000),\n",
        "              }   \n",
        "\n",
        "    grad = GradientBoostingRegressor()\n",
        "    final = RandomizedSearchCV(grad, param_distributions = params, cv = 5, scoring = 'neg_mean_squared_error', n_iter = 1000, n_jobs = -1 ,random_state=0)\n",
        "    final.fit(features, target)\n",
        "    pred = final.predict(x_test)\n",
        "\n",
        "    print('Best Params:', final.best_params_)\n",
        "    print('Best Score:', np.sqrt(-1 *final.best_score_))\n",
        "    print('Predict RMSE:',(np.sqrt(mean_squared_error(y_test, pred))))\n",
        "\n",
        "  elif model == 'xgb':\n",
        "    \n",
        "    params = {'n_estimators' : randint(30, 1000),\n",
        "              'learning_rate' :(0.01, 0.0001, 0.003, 0.5, 0.04, 0.008, 0.001, 0.351, 0.096, 0.853, 0.185, 0.01825, 0.012385, 0.1),\n",
        "              'max_depth' : randint(1, 1000),\n",
        "              'min_child_weight' : randint(2, 1000),\n",
        "              }   \n",
        "\n",
        "    xgb = XGBRegressor()\n",
        "    final = RandomizedSearchCV(xgb, param_distributions = params, cv = 5, scoring = 'neg_mean_squared_error', n_iter = 1000, n_jobs = -1 ,random_state=0)\n",
        "    final.fit(features, target)\n",
        "\n",
        "    pred = final.predict(x_test)\n",
        "\n",
        "    print('Best Params:', final.best_params_)\n",
        "    print('Best Score:', np.sqrt(-1 *final.best_score_))\n",
        "    print('Predict RMSE:',(np.sqrt(mean_squared_error(y_test, pred))))\n",
        "\n",
        "  elif model == 'lgbm':\n",
        "    params = {'n_estimators' : randint(1, 1000),\n",
        "              'learning_rate' :(0.01, 0.0001, 0.003, 0.5, 0.04, 0.008, 0.001, 0.351, 0.096, 0.853, 0.185, 0.01825, 0.012385, 0.1),\n",
        "              'max_depth' : randint(-1, 10),\n",
        "              'min_child_weight' : (0.001, 0.01, 0.5, 0.005, 0.0038, 0.001856, 0.0811, 0.1, 0.0931, 0.9, 1),\n",
        "              'num_leaves': randint(3, 100),\n",
        "              'min_child_samples':randint(1, 100)\n",
        "              }   \n",
        "\n",
        "    lgbm = LGBMRegressor()\n",
        "    final = RandomizedSearchCV(lgbm, param_distributions = params, cv = 5, scoring = 'neg_mean_squared_error', n_iter = 1000, n_jobs = -1 ,random_state=0)\n",
        "    final.fit(features, target)\n",
        "\n",
        "    pred = final.predict(x_test)\n",
        "\n",
        "    print('Best Params:', final.best_params_)\n",
        "    print('Best Score:', np.sqrt(-1 *final.best_score_))\n",
        "    print('Predict RMSE:',(np.sqrt(mean_squared_error(y_test, pred))))"
      ],
      "execution_count": 26,
      "outputs": []
    }
  ]
}