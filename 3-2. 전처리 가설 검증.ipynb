{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"border:1px solid gray\">\n",
    "    <thead style=\"border:1px solid gray\">\n",
    "        <tr>\n",
    "            <th>index</th>\n",
    "            <th>df명</th>\n",
    "            <th>설명</th>\n",
    "            <th>비교</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>0</td>\n",
    "            <td>df0_base</td>\n",
    "            <td>최소한의 전처리만 진행</td>\n",
    "            <td>다른 dataframe 검증할 때 비교</td>\n",
    "        </tr>\n",
    "        <tr style=\"border:1px solid gray\">\n",
    "            <td colspan=4 style=\"text-align: center\"><b>Date</b></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>1</td>\n",
    "            <td>df1_date</td>\n",
    "            <td>날짜 데이터 포함</td>\n",
    "            <td rowspan=3>df0_base, df1_date, df3_date 비교 후,<br>데이터 추가 후 df2_date와 비교</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>2</td>\n",
    "            <td>df2_date</td>\n",
    "            <td>날짜 데이터 제거</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>3</td>\n",
    "            <td>df3_date</td>\n",
    "            <td>날짜 데이터 one hot encoding</td>\n",
    "        </tr>\n",
    "        <tr style=\"border:1px solid gray\">\n",
    "            <td colspan=4 style=\"text-align: center\"><b>Adding columns</b></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>4</td>\n",
    "            <td>df4_name</td>\n",
    "            <td>어종별 수 컬럼 추가</td>\n",
    "            <td rowspan=2>df0_base<br>df4_name<br>df5_import</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>5</td>\n",
    "            <td>df5_import</td>\n",
    "            <td>수입용도 수 컬럼 추가</td>\n",
    "        </tr>\n",
    "        <tr style=\"border:1px solid gray\">\n",
    "            <td colspan=4 style=\"text-align: center\"><b>Dropping Columns</b></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>6</td>\n",
    "            <td>df6_caegory</td>\n",
    "            <td>CATEGORY_2, P_NAME 컬럼 제거</td>\n",
    "            <td rowspan=2>df0_base<br>df6_category<br>df7_category</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>7</td>\n",
    "            <td>df7_category</td>\n",
    "            <td>CATEGORY_1, CATEGORY_2 컬럼 제거</td>\n",
    "        </tr>\n",
    "        <tr style=\"border:1px solid gray\">\n",
    "            <td colspan=4 style=\"text-align: center\"><b>Adding Data</b></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>8</td>\n",
    "            <td>df8_add</td>\n",
    "            <td>제조국 날씨, 한국 날씨, 원유 종가, 염도, 소비자물가지수 data 추가</td>\n",
    "            <td>feature 중요도 파악<br>상관관계 파악</td>\n",
    "        </tr>\n",
    "        <tr style=\"border:1px solid gray\">\n",
    "            <td colspan=4 style=\"text-align: center\"><b>Normalization</b></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>9</td>\n",
    "            <td>df9_add</td>\n",
    "            <td>numerical feature 에 대한 정규화 진행</td>\n",
    "            <td>df0_base<br>feature 중요도 파악<br>상관관계 파악</td>\n",
    "        </tr>\n",
    "        <tr style=\"border:1px solid gray\">\n",
    "            <td colspan=4 style=\"text-align: center\"><b>Filling Missing Values</b></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>10</td>\n",
    "            <td>df10_fillna</td>\n",
    "            <td>fillna(method=ffill)로 날씨 결측치 처리</td>\n",
    "            <td rowspan=2>df0_base<br>df10_fillna<br>df11_fillna</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>11</td>\n",
    "            <td>df11_fillna</td>\n",
    "            <td>fillna(method=bfill)로 날씨 결측치 처리</td>\n",
    "        </tr>\n",
    "        <tr style=\"border:1px solid gray\">\n",
    "            <td colspan=4 style=\"text-align: center\"><b>Processing Outliers</b></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>12</td>\n",
    "            <td>df12_outlier</td>\n",
    "            <td>전체 날씨 데이터 정규화 후 outlier 일괄 제거</td>\n",
    "            <td rowspan=2>df0_base<br>df12_outlier<br>df13_outlier</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>13</td>\n",
    "            <td>df13_outlier</td>\n",
    "            <td>나라별로 최저/최고 기온 파악 후 outlier 개별 제거</td>\n",
    "        </tr>\n",
    "        <tr style=\"border:1px solid gray\">\n",
    "            <td colspan=4 style=\"text-align: center\"><b>Grouping</b></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>13</td>\n",
    "            <td></td>\n",
    "            <td></td>\n",
    "            <td></td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리 가설 검증\n",
    "- df_train + weather/oil 제외 추가데이터\n",
    "- 모든 나라에 대해서 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "# Visuzliation Setting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "from matplotlib import rc\n",
    "from matplotlib import colors\n",
    "import seaborn as sns\n",
    "\n",
    "# Modeling\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "FODjTCUA-j7r"
   },
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "\n",
    "\n",
    "def set_week(df, date):\n",
    "    '''\n",
    "    df : datetime 형식의 컬럼을 가지고 있는 dataframe\n",
    "    date : df에서 datetime 형식을 가진 컬럼명\n",
    "    return : date의 연도 컬럼과 주차 컬럼을 추가한 dataframe\n",
    "    '''\n",
    "    df[date] = pd.to_datetime(df[date])\n",
    "    df[date] = df[date].dt.date\n",
    "    df['year'] = df.apply(func=lambda x: x[date].isocalendar()[0], axis=1)\n",
    "    df['week'] = df.apply(func=lambda x: x[date].isocalendar()[1], axis=1)\n",
    "    df.drop(date, axis=1, inplace=True)\n",
    "    \n",
    "\n",
    "def check_week(df):\n",
    "    '''\n",
    "    df에 date가 전부 있는지 확인\n",
    "    '''\n",
    "    cnt = 0\n",
    "    sdate = date(2015, 12, 28)   # start date\n",
    "    edate = date(2019, 12, 30)   # end date\n",
    "    delta = edate - sdate       # as timedelta\n",
    "    mem = set()\n",
    "    \n",
    "    for i in range(delta.days + 1):\n",
    "        day = sdate + timedelta(days=i)\n",
    "        year, week = day.isocalendar()[0], day.isocalendar()[1]\n",
    "        if year * 100 + week in mem:\n",
    "            continue\n",
    "        mem.add(year * 100 + week)\n",
    "        if df[(df['year'] == year) & (df['week'] == week)].empty:\n",
    "            print((year, week), end=\"\")\n",
    "            cnt += 1\n",
    "    if cnt > 0:\n",
    "        print()\n",
    "    print(\"missing\", cnt, \"values\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(y, y_pred):\n",
    "    return mean_squared_error(y, y_pred)**0.5\n",
    "\n",
    "\n",
    "def train_model(train_data, target_data, model=LinearRegression()):  # baseline model : LInearRegression\n",
    "    x_train, x_test, y_train, y_test = train_test_split(train_data, target_data, random_state=0)\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "    print(\"Model Training Complete!\")\n",
    "\n",
    "    pred_train, pred_test = model.predict(x_train), model.predict(x_test)\n",
    "\n",
    "    plt.scatter(pred_train, y_train, s=10)\n",
    "    plt.xlabel(\"Predicted price\")\n",
    "    plt.ylabel(\"Actual price\")\n",
    "    plt.show()\n",
    "\n",
    "    # cvs = cross_val_score(model, x_test, y_test, cv = 5)\n",
    "    # print(\">> cross_val_score mean =\", cvs.mean())\n",
    "    print(\">> RMSE train =\", RMSE(y_train, pred_train))\n",
    "    print(\">> RMSE validation =\", RMSE(y_test, pred_test))\n",
    "    print(\">> MAE train =\", mean_absolute_error(pred_train, y_train))\n",
    "    print(\">> MAE validation =\", mean_absolute_error(pred_test, y_test))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.path.join(os.getcwd(), 'DATA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_df(df_list):\n",
    "    return reduce(lambda  left,right: pd.merge(left,right, on=['year', 'week'], how='left'), data_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_excel(os.path.join(root, 'train.xlsx'))\n",
    "df_oil = pd.read_csv(os.path.join(root, 'preprocessed_oil.csv'))\n",
    "df_weather_kr = pd.read_csv(os.path.join(root, 'preprocessed_weather_korea.csv'))\n",
    "df_cpi = pd.read_csv(os.path.join(root, 'preprocessed_cpi.csv'))\n",
    "df_exchange = pd.read_csv(os.path.join(root, 'preprocessed_exchange.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# df0_base\n",
    "**df_train**:  \n",
    "- 'REG_DATE' to 'year', 'week'\n",
    "- One hot encoding on categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0_base = df_train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GgRtMPI7TKm"
   },
   "source": [
    "### Add Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "VvRUXfdN-j7u"
   },
   "outputs": [],
   "source": [
    "set_week(df0_base, 'REG_DATE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2015-12-28T00:00:00.000000000', '2016-01-04T00:00:00.000000000',\n",
       "       '2016-01-11T00:00:00.000000000', '2016-01-18T00:00:00.000000000',\n",
       "       '2016-01-25T00:00:00.000000000', '2016-02-01T00:00:00.000000000',\n",
       "       '2016-02-08T00:00:00.000000000', '2016-02-15T00:00:00.000000000',\n",
       "       '2016-02-22T00:00:00.000000000', '2016-02-29T00:00:00.000000000',\n",
       "       '2016-03-07T00:00:00.000000000', '2016-03-14T00:00:00.000000000',\n",
       "       '2016-03-21T00:00:00.000000000', '2016-03-28T00:00:00.000000000',\n",
       "       '2016-04-04T00:00:00.000000000', '2016-04-11T00:00:00.000000000',\n",
       "       '2016-04-18T00:00:00.000000000', '2016-04-25T00:00:00.000000000',\n",
       "       '2016-05-02T00:00:00.000000000', '2016-05-09T00:00:00.000000000',\n",
       "       '2016-05-16T00:00:00.000000000', '2016-05-23T00:00:00.000000000',\n",
       "       '2016-05-30T00:00:00.000000000', '2016-06-06T00:00:00.000000000',\n",
       "       '2016-06-13T00:00:00.000000000', '2016-06-20T00:00:00.000000000',\n",
       "       '2016-06-27T00:00:00.000000000', '2016-07-04T00:00:00.000000000',\n",
       "       '2016-07-11T00:00:00.000000000', '2016-07-18T00:00:00.000000000',\n",
       "       '2016-07-25T00:00:00.000000000', '2016-08-01T00:00:00.000000000',\n",
       "       '2016-08-08T00:00:00.000000000', '2016-08-15T00:00:00.000000000',\n",
       "       '2016-08-22T00:00:00.000000000', '2016-08-29T00:00:00.000000000',\n",
       "       '2016-09-05T00:00:00.000000000', '2016-09-12T00:00:00.000000000',\n",
       "       '2016-09-19T00:00:00.000000000', '2016-09-26T00:00:00.000000000',\n",
       "       '2016-10-03T00:00:00.000000000', '2016-10-10T00:00:00.000000000',\n",
       "       '2016-10-17T00:00:00.000000000', '2016-10-24T00:00:00.000000000',\n",
       "       '2016-10-31T00:00:00.000000000', '2016-11-07T00:00:00.000000000',\n",
       "       '2016-11-14T00:00:00.000000000', '2016-11-21T00:00:00.000000000',\n",
       "       '2016-11-28T00:00:00.000000000', '2016-12-05T00:00:00.000000000',\n",
       "       '2016-12-12T00:00:00.000000000', '2016-12-19T00:00:00.000000000',\n",
       "       '2016-12-26T00:00:00.000000000', '2017-01-01T00:00:00.000000000',\n",
       "       '2017-01-06T00:00:00.000000000', '2017-01-16T00:00:00.000000000',\n",
       "       '2017-01-23T00:00:00.000000000', '2017-01-30T00:00:00.000000000',\n",
       "       '2017-02-06T00:00:00.000000000', '2017-02-13T00:00:00.000000000',\n",
       "       '2017-02-20T00:00:00.000000000', '2017-02-27T00:00:00.000000000',\n",
       "       '2017-03-06T00:00:00.000000000', '2017-03-13T00:00:00.000000000',\n",
       "       '2017-03-20T00:00:00.000000000', '2017-03-27T00:00:00.000000000',\n",
       "       '2017-04-03T00:00:00.000000000', '2017-04-10T00:00:00.000000000',\n",
       "       '2017-04-17T00:00:00.000000000', '2017-04-24T00:00:00.000000000',\n",
       "       '2017-05-01T00:00:00.000000000', '2017-05-08T00:00:00.000000000',\n",
       "       '2017-05-15T00:00:00.000000000', '2017-05-22T00:00:00.000000000',\n",
       "       '2017-05-29T00:00:00.000000000', '2017-06-05T00:00:00.000000000',\n",
       "       '2017-06-12T00:00:00.000000000', '2017-06-19T00:00:00.000000000',\n",
       "       '2017-06-26T00:00:00.000000000', '2017-07-03T00:00:00.000000000',\n",
       "       '2017-07-10T00:00:00.000000000', '2017-07-17T00:00:00.000000000',\n",
       "       '2017-07-24T00:00:00.000000000', '2017-07-31T00:00:00.000000000',\n",
       "       '2017-08-07T00:00:00.000000000', '2017-08-14T00:00:00.000000000',\n",
       "       '2017-08-21T00:00:00.000000000', '2017-08-28T00:00:00.000000000',\n",
       "       '2017-09-04T00:00:00.000000000', '2017-09-11T00:00:00.000000000',\n",
       "       '2017-09-18T00:00:00.000000000', '2017-09-25T00:00:00.000000000',\n",
       "       '2017-10-02T00:00:00.000000000', '2017-10-09T00:00:00.000000000',\n",
       "       '2017-10-16T00:00:00.000000000', '2017-10-23T00:00:00.000000000',\n",
       "       '2017-10-30T00:00:00.000000000', '2017-11-06T00:00:00.000000000',\n",
       "       '2017-11-13T00:00:00.000000000', '2017-11-20T00:00:00.000000000',\n",
       "       '2017-11-27T00:00:00.000000000', '2017-12-04T00:00:00.000000000',\n",
       "       '2017-12-11T00:00:00.000000000', '2017-12-18T00:00:00.000000000',\n",
       "       '2017-12-25T00:00:00.000000000', '2018-01-01T00:00:00.000000000',\n",
       "       '2018-01-08T00:00:00.000000000', '2018-01-15T00:00:00.000000000',\n",
       "       '2018-01-22T00:00:00.000000000', '2018-01-29T00:00:00.000000000',\n",
       "       '2018-02-05T00:00:00.000000000', '2018-02-12T00:00:00.000000000',\n",
       "       '2018-02-19T00:00:00.000000000', '2018-02-26T00:00:00.000000000',\n",
       "       '2018-03-05T00:00:00.000000000', '2018-03-12T00:00:00.000000000',\n",
       "       '2018-03-19T00:00:00.000000000', '2018-03-26T00:00:00.000000000',\n",
       "       '2018-04-02T00:00:00.000000000', '2018-04-09T00:00:00.000000000',\n",
       "       '2018-04-16T00:00:00.000000000', '2018-04-23T00:00:00.000000000',\n",
       "       '2018-04-30T00:00:00.000000000', '2018-05-07T00:00:00.000000000',\n",
       "       '2018-05-14T00:00:00.000000000', '2018-05-21T00:00:00.000000000',\n",
       "       '2018-05-28T00:00:00.000000000', '2018-06-04T00:00:00.000000000',\n",
       "       '2018-06-11T00:00:00.000000000', '2018-06-18T00:00:00.000000000',\n",
       "       '2018-06-25T00:00:00.000000000', '2018-07-02T00:00:00.000000000',\n",
       "       '2018-07-09T00:00:00.000000000', '2018-07-16T00:00:00.000000000',\n",
       "       '2018-07-23T00:00:00.000000000', '2018-07-30T00:00:00.000000000',\n",
       "       '2018-08-06T00:00:00.000000000', '2018-08-13T00:00:00.000000000',\n",
       "       '2018-08-20T00:00:00.000000000', '2018-08-27T00:00:00.000000000',\n",
       "       '2018-09-03T00:00:00.000000000', '2018-09-10T00:00:00.000000000',\n",
       "       '2018-09-17T00:00:00.000000000', '2018-09-24T00:00:00.000000000',\n",
       "       '2018-10-01T00:00:00.000000000', '2018-10-08T00:00:00.000000000',\n",
       "       '2018-10-15T00:00:00.000000000', '2018-10-22T00:00:00.000000000',\n",
       "       '2018-10-29T00:00:00.000000000', '2018-11-05T00:00:00.000000000',\n",
       "       '2018-11-12T00:00:00.000000000', '2018-11-19T00:00:00.000000000',\n",
       "       '2018-11-26T00:00:00.000000000', '2018-12-03T00:00:00.000000000',\n",
       "       '2018-12-10T00:00:00.000000000', '2018-12-17T00:00:00.000000000',\n",
       "       '2018-12-24T00:00:00.000000000', '2018-12-31T00:00:00.000000000',\n",
       "       '2019-01-07T00:00:00.000000000', '2019-01-14T00:00:00.000000000',\n",
       "       '2019-01-21T00:00:00.000000000', '2019-01-28T00:00:00.000000000',\n",
       "       '2019-02-04T00:00:00.000000000', '2019-02-11T00:00:00.000000000',\n",
       "       '2019-02-18T00:00:00.000000000', '2019-02-25T00:00:00.000000000',\n",
       "       '2019-03-04T00:00:00.000000000', '2019-03-11T00:00:00.000000000',\n",
       "       '2019-03-18T00:00:00.000000000', '2019-03-25T00:00:00.000000000',\n",
       "       '2019-04-01T00:00:00.000000000', '2019-04-08T00:00:00.000000000',\n",
       "       '2019-04-15T00:00:00.000000000', '2019-04-22T00:00:00.000000000',\n",
       "       '2019-04-29T00:00:00.000000000', '2019-05-06T00:00:00.000000000',\n",
       "       '2019-05-13T00:00:00.000000000', '2019-05-20T00:00:00.000000000',\n",
       "       '2019-05-27T00:00:00.000000000', '2019-06-03T00:00:00.000000000',\n",
       "       '2019-06-10T00:00:00.000000000', '2019-06-17T00:00:00.000000000',\n",
       "       '2019-06-24T00:00:00.000000000', '2019-07-01T00:00:00.000000000',\n",
       "       '2019-07-08T00:00:00.000000000', '2019-07-15T00:00:00.000000000',\n",
       "       '2019-07-22T00:00:00.000000000', '2019-07-29T00:00:00.000000000',\n",
       "       '2019-08-05T00:00:00.000000000', '2019-08-12T00:00:00.000000000',\n",
       "       '2019-08-19T00:00:00.000000000', '2019-08-26T00:00:00.000000000',\n",
       "       '2019-09-02T00:00:00.000000000', '2019-09-09T00:00:00.000000000',\n",
       "       '2019-09-16T00:00:00.000000000', '2019-09-23T00:00:00.000000000',\n",
       "       '2019-09-30T00:00:00.000000000', '2019-10-07T00:00:00.000000000',\n",
       "       '2019-10-14T00:00:00.000000000', '2019-10-21T00:00:00.000000000',\n",
       "       '2019-10-28T00:00:00.000000000', '2019-11-04T00:00:00.000000000',\n",
       "       '2019-11-11T00:00:00.000000000', '2019-11-18T00:00:00.000000000',\n",
       "       '2019-11-25T00:00:00.000000000', '2019-12-02T00:00:00.000000000',\n",
       "       '2019-12-09T00:00:00.000000000', '2019-12-16T00:00:00.000000000',\n",
       "       '2019-12-23T00:00:00.000000000', '2019-12-30T00:00:00.000000000'],\n",
       "      dtype='datetime64[ns]')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['REG_DATE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REG_DATE</th>\n",
       "      <th>P_TYPE</th>\n",
       "      <th>CTRY_1</th>\n",
       "      <th>CTRY_2</th>\n",
       "      <th>P_PURPOSE</th>\n",
       "      <th>CATEGORY_1</th>\n",
       "      <th>CATEGORY_2</th>\n",
       "      <th>P_NAME</th>\n",
       "      <th>P_IMPORT_TYPE</th>\n",
       "      <th>P_PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10285</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>수산물</td>\n",
       "      <td>아르헨티나</td>\n",
       "      <td>아르헨티나</td>\n",
       "      <td>판매용</td>\n",
       "      <td>갑각류</td>\n",
       "      <td>새우</td>\n",
       "      <td>아르헨티나붉은새우</td>\n",
       "      <td>냉동</td>\n",
       "      <td>2.700018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10286</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>수산물</td>\n",
       "      <td>방글라데시</td>\n",
       "      <td>방글라데시</td>\n",
       "      <td>판매용</td>\n",
       "      <td>어류</td>\n",
       "      <td>메기 동자개</td>\n",
       "      <td>메기</td>\n",
       "      <td>냉동</td>\n",
       "      <td>2.855192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10287</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>수산물</td>\n",
       "      <td>바레인</td>\n",
       "      <td>바레인</td>\n",
       "      <td>판매용</td>\n",
       "      <td>갑각류</td>\n",
       "      <td>게</td>\n",
       "      <td>꽃게</td>\n",
       "      <td>냉동</td>\n",
       "      <td>2.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10288</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>수산물</td>\n",
       "      <td>캐나다</td>\n",
       "      <td>캐나다</td>\n",
       "      <td>판매용</td>\n",
       "      <td>어류</td>\n",
       "      <td>연어</td>\n",
       "      <td>연어</td>\n",
       "      <td>냉장</td>\n",
       "      <td>11.482630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10289</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>수산물</td>\n",
       "      <td>중국</td>\n",
       "      <td>중국</td>\n",
       "      <td>판매용</td>\n",
       "      <td>어류</td>\n",
       "      <td>민물붕어</td>\n",
       "      <td>붕어</td>\n",
       "      <td>활</td>\n",
       "      <td>2.949800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10644</th>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>수산물</td>\n",
       "      <td>러시아</td>\n",
       "      <td>중국</td>\n",
       "      <td>판매용</td>\n",
       "      <td>어류</td>\n",
       "      <td>명태</td>\n",
       "      <td>명태</td>\n",
       "      <td>냉동,필렛(F)</td>\n",
       "      <td>2.704495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10645</th>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>수산물</td>\n",
       "      <td>중국</td>\n",
       "      <td>중국</td>\n",
       "      <td>판매용</td>\n",
       "      <td>패류 멍게류</td>\n",
       "      <td>조개</td>\n",
       "      <td>개조개</td>\n",
       "      <td>활</td>\n",
       "      <td>1.068850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10646</th>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>수산물</td>\n",
       "      <td>베트남</td>\n",
       "      <td>베트남</td>\n",
       "      <td>판매용</td>\n",
       "      <td>연체류 해물모듬</td>\n",
       "      <td>쭈꾸미</td>\n",
       "      <td>주꾸미</td>\n",
       "      <td>냉동</td>\n",
       "      <td>4.523705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10647</th>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>수산물</td>\n",
       "      <td>캐나다</td>\n",
       "      <td>캐나다</td>\n",
       "      <td>판매용</td>\n",
       "      <td>갑각류</td>\n",
       "      <td>가재 랍스타</td>\n",
       "      <td>바다가재</td>\n",
       "      <td>활</td>\n",
       "      <td>26.516125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10648</th>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>수산물</td>\n",
       "      <td>노르웨이</td>\n",
       "      <td>노르웨이</td>\n",
       "      <td>판매용</td>\n",
       "      <td>어류</td>\n",
       "      <td>연어</td>\n",
       "      <td>연어</td>\n",
       "      <td>냉장</td>\n",
       "      <td>12.346819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>364 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        REG_DATE P_TYPE CTRY_1 CTRY_2 P_PURPOSE CATEGORY_1 CATEGORY_2  \\\n",
       "10285 2017-01-01    수산물  아르헨티나  아르헨티나       판매용        갑각류         새우   \n",
       "10286 2017-01-01    수산물  방글라데시  방글라데시       판매용         어류     메기 동자개   \n",
       "10287 2017-01-01    수산물    바레인    바레인       판매용        갑각류          게   \n",
       "10288 2017-01-01    수산물    캐나다    캐나다       판매용         어류         연어   \n",
       "10289 2017-01-01    수산물     중국     중국       판매용         어류       민물붕어   \n",
       "...          ...    ...    ...    ...       ...        ...        ...   \n",
       "10644 2017-01-06    수산물    러시아     중국       판매용         어류         명태   \n",
       "10645 2017-01-06    수산물     중국     중국       판매용     패류 멍게류         조개   \n",
       "10646 2017-01-06    수산물    베트남    베트남       판매용   연체류 해물모듬        쭈꾸미   \n",
       "10647 2017-01-06    수산물    캐나다    캐나다       판매용        갑각류     가재 랍스타   \n",
       "10648 2017-01-06    수산물   노르웨이   노르웨이       판매용         어류         연어   \n",
       "\n",
       "          P_NAME P_IMPORT_TYPE    P_PRICE  \n",
       "10285  아르헨티나붉은새우            냉동   2.700018  \n",
       "10286         메기            냉동   2.855192  \n",
       "10287         꽃게            냉동   2.950000  \n",
       "10288         연어            냉장  11.482630  \n",
       "10289         붕어             활   2.949800  \n",
       "...          ...           ...        ...  \n",
       "10644         명태      냉동,필렛(F)   2.704495  \n",
       "10645        개조개             활   1.068850  \n",
       "10646        주꾸미            냉동   4.523705  \n",
       "10647       바다가재             활  26.516125  \n",
       "10648         연어            냉장  12.346819  \n",
       "\n",
       "[364 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_train[(df_train['REG_DATE'].astype('str').str[:9] == '2017-01-0')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k34yDLwC-j7v",
    "outputId": "3ef05846-e9c7-4ea5-a3fb-d1f890ca4489"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2017, 2)\n",
      "missing 1 values\n"
     ]
    }
   ],
   "source": [
    "check_week(df0_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3OU0ctu7TKo"
   },
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "WfYJ0J9I7TKo"
   },
   "outputs": [],
   "source": [
    "# P_IMPORT_TYPE\n",
    "import_type_list = set()\n",
    "for tmp in df0_base.P_IMPORT_TYPE.unique():\n",
    "    for a in tmp.split(','):\n",
    "        import_type_list.add(a)\n",
    "for name in import_type_list:\n",
    "    df0_base[name] = 0\n",
    "    df0_base.loc[df0_base['P_IMPORT_TYPE'].str.contains(name, regex=False), name] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "D2wAIEY_7TKw"
   },
   "outputs": [],
   "source": [
    "# Others\n",
    "one_hot = ['CTRY_1', 'CTRY_2', 'P_PURPOSE', 'CATEGORY_1', 'CATEGORY_2', 'P_NAME']\n",
    "df0_base = pd.get_dummies(df0_base, columns=one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TjOq4PM7TKx"
   },
   "source": [
    "### Drop Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "02DqP-aR7TKx"
   },
   "outputs": [],
   "source": [
    "drop = ['P_TYPE', 'P_IMPORT_TYPE']\n",
    "df0_base.drop(columns = drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yr1Q_9kz7TKx"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
