{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_file.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4iUd_kgj4XT"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler, RobustScaler, MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, VotingRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "from scipy.stats import randint"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJUrO15m0zzU"
      },
      "source": [
        "def model_scaler(data, col, scaler = None):\n",
        "  \n",
        "  '''\n",
        "  정규화 함수\n",
        "  data : dataframe\n",
        "  column : P_PRICE\n",
        "  scaler : standard, robust, minmax, log\n",
        "\n",
        "  '''\n",
        " \n",
        "  features = data.drop(col, axis=1)\n",
        "  target = data[col]\n",
        "\n",
        "  if scaler == 'standard':\n",
        "    scaler = StandardScaler()\n",
        "    features = scaler.fit_transform(features)\n",
        "\n",
        "    return features, target\n",
        "\n",
        "  elif scaler == 'robust':\n",
        "    scaler = RobustScaler()\n",
        "    features = scaler.fit_transform(features)\n",
        "\n",
        "    return features, target\n",
        "\n",
        "  elif scaler == 'minmax':\n",
        "    scaler = MinMaxScaler()\n",
        "    features = scaler.fit_transform(features)\n",
        "\n",
        "    return features, target\n",
        "\n",
        "  elif scaler == 'log':\n",
        "    features = np.log1p(features)\n",
        "\n",
        "    return features, target\n",
        "\n",
        "################################################################################################################################################\n",
        "\n",
        "def model_train(data, col, scaler, model = None):\n",
        "\n",
        "  '''\n",
        "  \n",
        "  data : dataframe\n",
        "  column : P_PRICE\n",
        "  scaler : standard, robust, minmax, log\n",
        "  model_name : linear, ridge, lasso, elastic, decisiontree,\n",
        "               randomforest, ada, gradient, xgb, lgbm\n",
        "\n",
        "  '''\n",
        "\n",
        "  features, target = model_scaler(data, col, scaler)\n",
        "  x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0, random_state=0)\n",
        "  \n",
        "  if model == 'linear': \n",
        "    \n",
        "    model = LinearRegression()\n",
        "    neg_mse_scores = cross_val_score(lr, features, target, scoring = 'neg_mean_squared_error', cv = 5)\n",
        "    rmse_scores = np.sqrt(-1 * neg_mse_scores)\n",
        "    avg_rmse = np.mean(rmse_scores)\n",
        "\n",
        "\n",
        "    print('RMSE : {:.4f}'.format(rmse))\n",
        "\n",
        "  elif model == 'ridge':\n",
        "    \n",
        "    params = {\n",
        "              'alpha': randint(0, 10),            \n",
        "              'fit_intercept':(True, False),\n",
        "              'normalize':(True, False),\n",
        "\n",
        "              }\n",
        "\n",
        "    ridge = Ridge(random_state=0)\n",
        "    final = RandomizedSearchCV(rf, param_distributions = params, cv = 5, scoring = 'neg_mean_squared_error', n_iter = 10, n_jobs = -1 ,random_state=random_state)\n",
        "    final.fit(features, target)\n",
        "\n",
        "    print('Best Params:', final.best_params_)\n",
        "    print('Best Score:', np.sqrt(-1 *final.best_score_))\n",
        "\n",
        "    \n",
        "  elif model == 'lasso':\n",
        "\n",
        "    params = {\n",
        "              'alpha': randint(0, 10),            \n",
        "              'fit_intercept':(True, False),\n",
        "              'normalize':(True, False),\n",
        "\n",
        "              }\n",
        "\n",
        "    lasso = Lasso(random_state=0)\n",
        "    final = RandomizedSearchCV(lasso, param_distributions = params, cv = 5, scoring = 'neg_mean_squared_error', n_iter = 10, n_jobs = -1 ,random_state=random_state)\n",
        "    final.fit(features, target)\n",
        "\n",
        "    print('Best Params:', final.best_params_)\n",
        "    print('Best Score:', np.sqrt(-1 *final.best_score_))\n",
        "  \n",
        "  elif model == 'elastic':\n",
        "    pass\n",
        "\n",
        "  elif model == 'decisiontree':\n",
        "    \n",
        "    params = {\n",
        "              'max_depth': randint(10, 100),            \n",
        "              'n_estimators':randint(100, 1000),\n",
        "               #'min_child_samples': randint(5, 50),\n",
        "              'min_samples_leaf':randint(1, 10),\n",
        "              'min_samples_split': randint(1, 10),\n",
        "              'max_leaf_nodes': randint(2, 10)\n",
        "              }\n",
        "\n",
        "    dt = DecisionTreeRegressor(random_state=0)\n",
        "    final = RandomizedSearchCV(dt, param_distributions = params, cv = 5, scoring = 'neg_mean_squared_error', n_iter = 10, n_jobs = -1 ,random_state=random_state)\n",
        "    final.fit(features, target)\n",
        "\n",
        "    print('Best Params:', final.best_params_)\n",
        "    print('Best Score:', np.sqrt(-1 *final.best_score_))\n",
        "  \n",
        "  elif model == 'randomforest':\n",
        "    \n",
        "    params = {\n",
        "              'max_depth': randint(30, 100),            \n",
        "              'n_estimators':randint(100, 1000),\n",
        "               #'min_child_samples': randint(5, 50),\n",
        "              'min_samples_leaf':randint(1, 10),\n",
        "              'min_samples_split': randint(2, 10),\n",
        "              'max_leaf_nodes': randint(2, 10)\n",
        "\n",
        "              }\n",
        "\n",
        "    rf = RandomForestRegressor(random_state=0)\n",
        "    final = RandomizedSearchCV(rf, param_distributions = params, cv = 5, scoring = 'neg_mean_squared_error', n_iter = 10, n_jobs = -1 ,random_state=random_state)\n",
        "    final.fit(features, target)\n",
        "\n",
        "    print('Best Params:', final.best_params_)\n",
        "    print('Best Score:', np.sqrt(-1 *final.best_score_))\n",
        "\n",
        "  elif model == 'ada':\n",
        "    \n",
        "    params = {'n_estimators' : randint(30, 1000),\n",
        "              'learning_rate' : randint(0.01 , 1)}\n",
        "\n",
        "    ada = AdaBoostRegressor()\n",
        "    final = RandomizedSearchCV(ada, param_distributions = params, cv = 5, scoring = 'neg_mean_squared_error', n_iter = 10, n_jobs = -1 ,random_state=random_state)\n",
        "    final.fit(features, target)\n",
        "\n",
        "    print('Best Params:', final.best_params_)\n",
        "    print('Best Score:', np.sqrt(-1 *final.best_score_))\n",
        "\n",
        "  elif model == 'gradinet':\n",
        "\n",
        "    params = {'n_estimators' : randint(30, 1000),\n",
        "              'learning_rate' : randint(0.01 , 1),\n",
        "              'subsample' : randint(0.01, 1),\n",
        "              'min_samples_split' : randint(2, 10),\n",
        "              'max_depth' : randint(3, 10),\n",
        "              }   \n",
        "\n",
        "    grad = GradientBoostingRegressor()\n",
        "    final = RandomizedSearchCV(grad, param_distributions = params, cv = 5, scoring = 'neg_mean_squared_error', n_iter = 10, n_jobs = -1 ,random_state=random_state)\n",
        "    final.fit(features, target)\n",
        "\n",
        "    print('Best Params:', final.best_params_)\n",
        "    print('Best Score:', np.sqrt(-1 *final.best_score_))\n",
        "\n",
        "  elif model == 'xgb':\n",
        "    \n",
        "    params = {'n_estimators' : randint(30, 1000),\n",
        "              'learning_rate' : randint(0.01 , 1),\n",
        "              'max_depth' : randint(1, 10),\n",
        "              'min_child_weight' : randint(2, 10),\n",
        "              }   \n",
        "\n",
        "    xgb = XGBRegressor()\n",
        "    final = RandomizedSearchCV(xgb, param_distributions = params, cv = 5, scoring = 'neg_mean_squared_error', n_iter = 10, n_jobs = -1 ,random_state=random_state)\n",
        "    final.fit(features, target)\n",
        "\n",
        "    print('Best Params:', final.best_params_)\n",
        "    print('Best Score:', np.sqrt(-1 *final.best_score_))\n",
        "\n",
        "  elif model == 'lgbm':\n",
        "    params = {'n_estimators' : randint(30, 1000),\n",
        "              'learning_rate' : randint(0.01 , 1),\n",
        "              'max_depth' : randint(-1, 10),\n",
        "              'min_child_weight' : randint(0.001, 1),\n",
        "              'num_leaves': randint(3, 100),\n",
        "              'min_child_samples':randint(1, 100)\n",
        "              }   \n",
        "\n",
        "    lgbm = XGBRegressor()\n",
        "    final = RandomizedSearchCV(lgbm, param_distributions = params, cv = 5, scoring = 'neg_mean_squared_error', n_iter = 10, n_jobs = -1 ,random_state=random_state)\n",
        "    final.fit(features, target)\n",
        "\n",
        "    print('Best Params:', final.best_params_)\n",
        "    print('Best Score:', np.sqrt(-1 *final.best_score_))"
      ],
      "execution_count": 2,
      "outputs": []
    }
  ]
}