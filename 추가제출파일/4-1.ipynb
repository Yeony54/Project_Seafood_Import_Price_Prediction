{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4-1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMRL5E1Ej4Ym2AZsk49rVOo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"5fvbqpjRGsEV"},"source":["# Import Libraries & Functions\n","from utility import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SzfvL1R2GhiQ","executionInfo":{"status":"ok","timestamp":1631636397974,"user_tz":-540,"elapsed":2070,"user":{"displayName":"Jaewoo Choi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgocACHs1NzyfJtDmltNZlRNmSS66E6ue7g-ZJE=s64","userId":"05968598841946004036"}}},"source":["# System Libraries\n","import os\n","import warnings\n","\n","\n","# Data handling Libraries\n","import pandas as pd\n","import numpy as np\n","\n","# Visuzliation Libraries\n","import matplotlib.pyplot as plt\n","import matplotlib.font_manager as fm\n","from matplotlib import rc\n","from matplotlib import colors\n","import seaborn as sns\n","\n","# Traning & Modeling\n","from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, VotingRegressor, GradientBoostingRegressor\n","from xgboost import XGBRegressor\n","from lightgbm import LGBMRegressor\n","from sklearn.model_selection import RandomizedSearchCV, train_test_split\n","from sklearn.model_selection import cross_val_score, GridSearchCV\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import mean_squared_error\n","from scipy.stats import norm\n","from sklearn.preprocessing import StandardScaler\n","from scipy import stats\n","from scipy.stats import randint\n","\n","# Date\n","from datetime import date, timedelta, datetime\n","from calendar import monthrange\n","\n","################################################################## Date\n","def set_week(df, date):\n","    \"\"\"\n","    dataframe 의 년월일 날짜 컬럼을 년 컬럼과 주차 컬럼으로 분리하는 함수\n","    :param df: datetime 형식의 컬럼을 가지고 있는 dataframe\n","    :param date: df에서 datetime 형식을 가진 컬럼명\n","    :return: date의 연도 컬럼과 주차 컬럼을 추가한 dataframe\n","    \"\"\"\n","    df[date] = pd.to_datetime(df[date])\n","    df[date] = df[date].dt.date\n","    df['year'] = df.apply(func=lambda x: x[date].isocalendar()[0], axis=1)\n","    df['week'] = df.apply(func=lambda x: x[date].isocalendar()[1], axis=1)\n","    df.drop(date, axis=1, inplace=True)\n","\n","\n","def check_week(df):\n","    \"\"\"\n","    dataframe에 sdate 과 edate 사이에 모든 데이터가 있는지 확인하는 함수\n","    :param df: 검사하고자 하는 dataframe (set_week 형태)\n","    :return: None\n","    \"\"\"\n","    cnt = 0\n","    sdate = date(2015, 12, 28)  # start date\n","    edate = date(2019, 12, 30)  # end date\n","    delta = edate - sdate  # as timedelta\n","    mem = set()\n","\n","    for i in range(delta.days + 1):\n","        day = sdate + timedelta(days=i)\n","        year, week = day.isocalendar()[0], day.isocalendar()[1]\n","        if year * 100 + week in mem:\n","            continue\n","        mem.add(year * 100 + week)\n","        if df[(df['year'] == year) & (df['week'] == week)].empty:\n","            print((year, week), end=\"\")\n","            cnt += 1\n","    if cnt > 0:\n","        print()\n","    print(\"missing\", cnt, \"values\")\n","\n","    \n","################################################################## Wrangling Hypothesis Validation Functions\n","def RMSE(y, y_pred):\n","    return mean_squared_error(y, y_pred) ** 0.5\n","\n","\n","def train_model(train_data, target_data, model=LinearRegression()):\n","    \"\"\"\n","    주어진 model로 train_data와 target_data를 훈련하는 함수\n","    :param train_data: data for training model\n","    :param target_data: target value to be predicted\n","    :param model: model to train (default: Linear Regression Model)\n","    :return model: trained model\n","    :return g: regression plot\n","    \"\"\"\n","    x_train, x_test, y_train, y_test = train_test_split(train_data, target_data, random_state=0)\n","\n","    model.fit(x_train, y_train)\n","    print(\"Model Training Complete!\")\n","\n","    pred_train, pred_test = model.predict(x_train), model.predict(x_test)\n","\n","    g = plt.figure(figsize=(10, 8))\n","    sns.regplot(pred_train, y_train, scatter_kws = {'color': '#4CB7D8', 'alpha': 0.4}, line_kws = {'color': '#141D50', 'alpha': 0.8})\n","    plt.xlabel(\"Predicted price\")\n","    plt.ylabel(\"Actual price\")\n","    plt.show()\n","\n","    print(\">> RMSE train =\", RMSE(y_train, pred_train))\n","    print(\">> RMSE validation =\", RMSE(y_test, pred_test))\n","#     print(\">> MAE train =\", mean_absolute_error(pred_train, y_train))\n","#     print(\">> MAE validation =\", mean_absolute_error(pred_test, y_test))\n","    print(\"-------------------------------------------------\")\n","\n","    return model, g\n","\n","\n","def print_importance(model, df, added_columns):\n","    importance = model.coef_\n","    fs_data = []\n","    for i, x in enumerate(importance):\n","        fs_data.append([abs(x), df.columns[i]])\n","    fs_data.sort(key=lambda x: x[0], reverse=True)\n","\n","    # 추가한 컬럼의 중요도\n","    for i in range(len(fs_data)):\n","        if fs_data[i][1] in added_columns:\n","            print(fs_data[i][1], \":\", fs_data[i][0], \">\", i, \"순위\")\n","    print(\"-------------------------------------------------\")\n","    print(\"총\", len(fs_data), \"개\")\n","\n","    return fs_data\n","\n","\n","def model_scaler(data, col, scaler=None):\n","    \"\"\"\n","    정규화 함수\n","    data : dataframe\n","    column : P_PRICE\n","    scaler : standard, robust, minmax, log\n","    \"\"\"\n","\n","    features = data.drop(col, axis=1)\n","    target = data[col]\n","\n","    if scaler == 'standard':\n","        scaler = StandardScaler()\n","        features = scaler.fit_transform(features)\n","\n","        return features, target\n","\n","    elif scaler == 'robust':\n","        scaler = RobustScaler()\n","        features = scaler.fit_transform(features)\n","\n","        return features, target\n","\n","    elif scaler == 'minmax':\n","        scaler = MinMaxScaler()\n","        features = scaler.fit_transform(features)\n","\n","        return features, target\n","\n","    elif scaler == 'log':\n","        features = np.log1p(features)\n","\n","        return features, target\n","\n","    elif scaler == 'None':\n","\n","        return features, target\n","\n","################################################################## Save images\n","def save_img(plt, img_name):\n","    \"\"\"\n","    주어진 그래프를 저장하는 함수\n","    :param plt: 저장하고자 하는 그래프\n","    :param img_name: 이미지 파일명\n","    :return: None\n","    \"\"\"\n","    plt.savefig(os.path.join(os.getcwd(), 'IMAGES', img_name + '.png'), transparent=True)\n","    return\n","\n","################################################################################################################################################\n","\n","def model_train(data, col, scaler, cv=5, n_iter=50, model=None):\n","    '''\n","    data : dataframe\n","    column : P_PRICE\n","    scaler : standard, robust, minmax, log, none\n","    model_name : linear, ridge, lasso, elastic, decisiontree,\n","                 randomforest, ada, gradient, xgb, lgbm\n","    '''\n","\n","    features, target = model_scaler(data, col, scaler)\n","    x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=0)\n","\n","    if model == 'linear':\n","\n","        model = LinearRegression()\n","        neg_mse_scores = cross_val_score(model, features, target, scoring='neg_mean_squared_error', cv=cv)\n","        rmse_scores = np.sqrt(-1 * neg_mse_scores)\n","        avg_rmse = np.mean(rmse_scores)\n","\n","        print('RMSE : {:.4f}'.format(avg_rmse))\n","\n","    elif model == 'ridge':\n","\n","        params = {\n","            'alpha': (\n","            0.01, 0.0001, 0.003, 0.5, 0.04, 0.008, 0.912, 0.098, 0.0625, 0.1763, 0.001, 0.351, 0.096, 0.853, 0.185,\n","            0.01825, 0.012385, 0.1, 1, 10, 100, 200, 50, 30, 20, 29, 58),\n","            'fit_intercept': (True, False),\n","            'normalize': (True, False),\n","\n","        }\n","\n","        ridge = Ridge(random_state=0)\n","        final = RandomizedSearchCV(ridge, param_distributions=params, cv=cv, scoring='neg_mean_squared_error',\n","                                   n_iter=n_iter, n_jobs=-1, random_state=0)\n","        final.fit(x_train, y_train)\n","        pred = final.predict(x_test)\n","\n","        print('Best Params:', final.best_params_)\n","        print('Best Score:', np.sqrt(-1 * final.best_score_))\n","        print('Predict RMSE:', (np.sqrt(mean_squared_error(y_test, pred))))\n","\n","    elif model == 'lasso':\n","\n","        params = {\n","            'alpha': (\n","            0.01, 0.0001, 0.003, 0.5, 0.04, 0.1734, 0.098, 0.0074, 0.0001, 0.00923, 0.98, 0.174, 0.008, 0.001, 0.351,\n","            0.096, 0.853, 0.185, 0.01825, 0.012385, 0.1, 1, 10, 100, 200, 50, 30, 20, 29, 58),\n","            'fit_intercept': (True, False),\n","            'normalize': (True, False),\n","\n","        }\n","\n","        lasso = Lasso(random_state=0)\n","        final = RandomizedSearchCV(lasso, param_distributions=params, cv=cv, scoring='neg_mean_squared_error',\n","                                   n_iter=n_iter, n_jobs=-1, random_state=0)\n","        final.fit(features, target)\n","        pred = final.predict(x_test)\n","\n","        print('Best Params:', final.best_params_)\n","        print('Best Score:', np.sqrt(-1 * final.best_score_))\n","        print('Predict RMSE:', (np.sqrt(mean_squared_error(y_test, pred))))\n","\n","    elif model == 'elastic':\n","\n","        params = {\n","            'alpha': (0.1, 0.01, 0.5, 1, 3, 5, 10),\n","            'l1_ratio': (0.01, 0.0001, 0.003, 0.5, 0.04, 0.008, 0.1, 0.0125, 0.98263, 0.0935)\n","        }\n","\n","        elastic = ElasticNet()\n","        final = RandomizedSearchCV(elastic, param_distributions=params, cv=cv, scoring='neg_mean_squared_error',\n","                                   n_iter=n_iter, n_jobs=-1, random_state=0)\n","        final.fit(features, target)\n","        pred = final.predict(x_test)\n","\n","        print('Best Params:', final.best_params_)\n","        print('Best Score:', np.sqrt(-1 * final.best_score_))\n","        print('Predict RMSE:', (np.sqrt(mean_squared_error(y_test, pred))))\n","    elif model == 'decisiontree':\n","\n","        params = {\n","            'max_depth': randint(10, 1000),\n","            # 'min_child_samples': randint(5, 50),\n","            'min_samples_split': randint(1, 1000),\n","            'min_samples_leaf': randint(1, 1000),\n","\n","        }\n","\n","        dt = DecisionTreeRegressor(random_state=0)\n","        final = RandomizedSearchCV(dt, param_distributions=params, cv=cv, scoring='neg_mean_squared_error',\n","                                   n_iter=n_iter, n_jobs=-1, random_state=0)\n","        final.fit(features, target)\n","        pred = final.predict(x_test)\n","\n","        print('Best Params:', final.best_params_)\n","        print('Best Score:', np.sqrt(-1 * final.best_score_))\n","        print('Predict RMSE:', (np.sqrt(mean_squared_error(y_test, pred))))\n","    elif model == 'randomforest':\n","\n","        params = {\n","            'max_depth': randint(1, 5000),\n","            'n_estimators': randint(1, 5000),\n","            # 'min_child_samples': randint(5, 50),\n","            'min_samples_leaf': randint(1, 5000),\n","            'min_samples_split': randint(1, 5000),\n","            'max_leaf_nodes': randint(1, 5000)\n","\n","        }\n","\n","        rf = RandomForestRegressor(random_state=0)\n","        final = RandomizedSearchCV(rf, param_distributions=params, cv=cv, scoring='neg_mean_squared_error',\n","                                   n_iter=n_iter, n_jobs=-1, random_state=0)\n","        final.fit(features, target)\n","        pred = final.predict(x_test)\n","\n","        print('Best Params:', final.best_params_)\n","        print('Best Score:', np.sqrt(-1 * final.best_score_))\n","        print('Predict RMSE:', (np.sqrt(mean_squared_error(y_test, pred))))\n","\n","\n","    elif model == 'gradient':\n","\n","        params = {'n_estimators': randint(30, 1000),\n","                  'learning_rate': (\n","                  0.01, 0.0001, 0.003, 0.5, 0.04, 0.008, 0.001, 0.351, 0.096, 0.853, 0.185, 0.01825, 0.012385, 0.1),\n","                  'subsample': (0.01, 0.1, 0.5, 0.08, 0.35, 0.3, 0.001, 0.03, 0.006, 0.153, 0.193, 0.0012, 0.0083, 1),\n","                  'min_samples_split': randint(1, 5000),\n","                  'max_depth': randint(1, 5000),\n","                  }\n","\n","        grad = GradientBoostingRegressor()\n","        final = RandomizedSearchCV(grad, param_distributions=params, cv=cv, scoring='neg_mean_squared_error',\n","                                   n_iter=n_iter, n_jobs=-1, random_state=0)\n","        final.fit(features, target)\n","        pred = final.predict(x_test)\n","\n","        print('Best Params:', final.best_params_)\n","        print('Best Score:', np.sqrt(-1 * final.best_score_))\n","        print('Predict RMSE:', (np.sqrt(mean_squared_error(y_test, pred))))\n","\n","    elif model == 'xgb':\n","\n","        params = {'n_estimators': randint(1, 5000),\n","                  'learning_rate': (\n","                  0.01, 0.0001, 0.003, 0.5, 0.04, 0.008, 0.001, 0.351, 0.096, 0.853, 0.185, 0.01825, 0.012385, 0.1),\n","                  'max_depth': randint(1, 1000),\n","                  'min_child_weight': randint(1, 5000),\n","                  }\n","\n","        xgb = XGBRegressor()\n","        final = RandomizedSearchCV(xgb, param_distributions=params, cv=cv, scoring='neg_mean_squared_error',\n","                                   n_iter=n_iter, n_jobs=-1, random_state=0)\n","        final.fit(features, target)\n","\n","        pred = final.predict(x_test)\n","\n","        print('Best Params:', final.best_params_)\n","        print('Best Score:', np.sqrt(-1 * final.best_score_))\n","        print('Predict RMSE:', (np.sqrt(mean_squared_error(y_test, pred))))\n","\n","    elif model == 'lgbm':\n","        params = {'n_estimators': randint(1, 5000),\n","                  'learning_rate': (\n","                  0.01, 0.0001, 0.003, 0.5, 0.04, 0.008, 0.001, 0.351, 0.096, 0.853, 0.185, 0.01825, 0.012385, 0.1),\n","                  'max_depth': randint(-1, 10),\n","                  'min_child_weight': (0.001, 0.01, 0.5, 0.005, 0.0038, 0.001856, 0.0811, 0.1, 0.0931, 0.9, 1),\n","                  'num_leaves': randint(3, 5000),\n","                  'min_child_samples': randint(1, 5000)\n","                  }\n","\n","        lgbm = LGBMRegressor()\n","        final = RandomizedSearchCV(lgbm, param_distributions=params, cv=cv, scoring='neg_mean_squared_error',\n","                                   n_iter=n_iter, n_jobs=-1, random_state=0)\n","        final.fit(features, target)\n","\n","        pred = final.predict(x_test)\n","\n","        print('Best Params:', final.best_params_)\n","        print('Best Score:', np.sqrt(-1 * final.best_score_))\n","        print('Predict RMSE:', (np.sqrt(mean_squared_error(y_test, pred))))"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"LrE_811FG5XM"},"source":["root = os.path.join(os.getcwd(), 'DATA')\n","model = pd.read_csv(os.path.join(root, 'preprocessed_train_1.csv'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T2Iorn-pHKJx"},"source":["model_train(model, 'P_PRICE', 'none', cv=5, n_iter=10, model='linear')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C5vrJLbdHK2O"},"source":["model_train(data, col, scaler, cv=5, n_iter=10, model='ridge')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QXxWvUs7HK0A"},"source":["model_train(data, col, scaler, cv=5, n_iter=10, model='lasso')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NAF1IFjXHKxS"},"source":["model_train(data, col, scaler, cv=5, n_iter=10, model='elastic')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wylykcjsHKus"},"source":["model_train(data, col, scaler, cv=5, n_iter=10, model='decisiontree')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cHB8VhenHeCG"},"source":["model_train(data, col, scaler, cv=5, n_iter=10, model='randomforest')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3JDDoA6IHd_f"},"source":["model_train(data, col, scaler, cv=5, n_iter=10, model='gradient')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_DiJ7XNHHd9C"},"source":["model_train(data, col, scaler, cv=5, n_iter=10, model='xgb')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qmmEeJBjHqaS"},"source":["model_train(data, col, scaler, cv=5, n_iter=10, model='lgbm')"],"execution_count":null,"outputs":[]}]}